Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2021-08-08T20:57:11+01:00

====== Dynamic topic modeling ======
Created Sunday 08 August 2021

This is a collection of jotting from reviewed journals in this topic.
There are state space model and epoch centric model.
longitudinal data is use to study the effect of something over time by following and observing the same set of objects overtime. Unlike cross-sectional data that compares different object.

dirichlet distribution
a distribution of k-simplex where an example of a simplex is (0.3,0.6,0.1). This example is a 3-dimensional simplex. To undestand dirichlet distributions imagine a set of every 3 real valued number that all add up to one  and each one has the probability of occuring.  This is a dirichlet distribution.

{{./pasted_image002.png}}

the parameters are  alpha as shown in the diagram above. the only strange symbol is the gamma function , gamma function is the extension of factorial to complex number   and real numbers for integers. gamma (n) = (n-1)! for complex numbers it is defined as:

{{./pasted_image003.png}}

Dirichlet process

A drichlet process is a distribution over probability measures (like simplex in dirichlet distribution) and it has two parameters, H, the base distibution, like the mean of the DP, α 

Traditional topic model algorithms assume documents are exchangeable, that is the order in which the document was created does not matter.

parametric topic modeling algorithm require a set of parameter , for example, LDA require that the  size of the topic should be fixed. Nonparametric methods does not suffer from this shortcomings. An example of nonparametric methods is Heirarchical dirichlet process (HDP)

**The factors to consider when dealing with topic models **(This is equivalent to factors to consider for sequential data such as time-series data)

1. **How the topic distribution evolves** ( this will involve the semantic change in meaning of words, as the meaning of words changes, it is possible that the word will be associated with different word than was associated , before and the strenght of those relationship can also change). This also mean word correlation

3. **Birth and end of topic ** (This is related to saying the size of the topic is dynamic, that is it is not fixed , that is topics can be added or removed)
4. **The determination of time epoch** (how well to space the training document. questions like does, separating document by year, month, or weeks appropriate for setting the right time epoch)
5. **Topic trend** (how the topic is increasing or decreasing , this can also have something to do with birth and death rate of topic and also related to topic distribution).  Another example is in  (finding scientific topic) where the popularity, regarded as the hotness and coldness of a topic is measured. The method used here does not model topic improvement over time, like in a state space model

6. **The dynamic  distribution of each of the component (document and words) : **how does this items change and how does the change affect the  topic generally.
7. **Topic breaking or merging over time (some for of heirarchical clustering) (I am yet to see a paper that is doing this)**
8. **Topic co-occurrence over time**  (this is like dynamic correlated topic modeling)
9. **Flexibility to model spike change in topic (bursty topics)**


Also, some DTM discretize time into some form of epoch, while other model time as continous, without any form of discretization like in TOT. will this model not have some form of performance implication?

**Time-varying dynamic topic model: a better tool for mining microblogs at a global level**

This paper make use of twitter microblog data and each was assume to contain a single topic based on the shortness. the main contribution is to make sure the number of topic is not fixed unlike some of the previous papers and also apply online (online Gibbs sampling) training of document as opposed to batch training.  

The paper also tries to make a separation between  personal and global post. Since microblogging are assumed to be about user personal preferences and thoughts.  The personal and global classification was achieved by  by comparing the topic with that of the user (publisher) long term of interest. if it matches, it is personal else it is global.

The following items were considered:

Vocabulary of the documents and timestamp to added to each of the post by the users. Also the users were noted. That is there is a fixed number of user denoted as U.
each post is marked as personal or global.

the global and personal tag enables the selection of 

-specific or user specific measure. personal is time-specific while global is for user specific

Other paper to consider from here:

**Diao, Jiang, Zhu & Lim (2012)** : TimeUserLDA



**IMS-DTM: Incremental Multi-Scale Dynamic Topic Models**

According to this paper, traditional dynamic topic modeling require specific time period. for example in the DTM by Blei,  the time period for this topic is a year, but this may not the case, topic might not change every year, it could be every two year or even less. Also, some topics might die off and while other topics might emerge. it is also difficult to specify a perfect epoch. Therefore this paper proposed a method to make the time epoch selection dynamic. This is made possible by the evidences collected from multiple time scale. The paper also proposed a way to factor in multiscale efficiency  through a modification of Gibb sampling. The method is called. Multi-Scale Dynamic Topic Model (IMS-DTM).

The documents here a split into different time scale each with varying epoch. therfore foreach timeslices, and the scales overlap with each other

OTHER PAPERS TO CONSIDER FROM HERE:

(Blei and McAuliffe 2007) — Some for of supervised DTM
(Wang,Blei, and Heckerman 2008) — treat time as continuous


QUESTIONS TO PONDER:
what does it mean by non-conjugacy of multinomial and gaussian
review the mathematical models in this paper


**Sequential Modeling of Topic Dynamics with Multiple Timescales**

proposed multi-scale dynamic topic models. This paper was built upon by the first IMS paper.

OTHER PAPERS TO CONSIDER

[Nallapati et al. 2007]

**A Temporal Topic Model for Noisy Mediums**

Based on graph model , the paper tries to remove the impact of domain specific words from topic.

This paper tries to track topic through time, by considering topic emergence,persistence and dimishing  and also employ a form of state space model by making use of previous topic for new topic. The paper also employ a new metric : energy-nutrition ratio to identify inmportant terms inthe semantic graph to reduce flood or domain word for building the topic and a more efficient graph transversal method.

The removal of flood word is important when analysing  some content specific topics. Defines terms like nutrition, energy and the nutrition-energy ratio.

The nutrition is an indicator of how important the aterm is in a document  collection.Energy consider the change in nutrition of document at particular time t. To correct the energy calculation baised onver higher nutrition term, aan energy-nutrition ratio is used.
The energy-nutrition ratio help determine if the rate is accelerating, decelearating or staying the same over time.

This does not use a probabistic model like LDA, I do not really understand the output.

OTHER PAPERS TO CONSIDER:
Cataldi et al.

**A Time-Dependent Topic Model for Multiple Text Streams**

focuse ont eh problem of multiple data source stream  such that each stream can have both local and shared topic.Each topic is also identified with a time dependent function that that determines the topic similarity over time.
The paper make use of Yahoo new and Twitter text streams. The model is based on LDA with a mdistribution of how the document will choose a local or common topics. Collapsed Gibbs sampling was used. 

look at further, but really there is no new or super novel method in this paper.

OTHER PAPERS TO CONSIDER:

Ahmed and Xing [1] — focus on the birth and death of topics

**Timeline: A Dynamic Hierarchical Dirichlet Process Model for**
**Recovering Birth/Death and Evolution of Topics in Text Stream**


The introduced module was termed infinite dynamic topic model (iDTM). epoch here means naturally grouped by time, for instance, yearly, based on conference or event , etc.

This paper tries to solve the limitations of other dynamic topic modeling and also model the birth and death of topic model. it improves on limitations such as fixed topic size for each epoch,  cluster based method that assume single topic per document, topic popularity and so on. Evolutions was captured at different level, for example, the word evolution according to first-order state space model and the topic popularity according to the rich-get richer scheme via differnece order process (poly urn). (urn of balls of different colors, such that if you pick a color the ball is return and another ball of the same color is added to the urn). This paper also proposed a sampling algorithm for speeding up the processing by caching the sufficient statistics 
The number of topic and the topic distribution can change over time. parameters of the model evolves in a markovian fashion.
the hyperperameters in the model are delta and lambda. delta is the width of the decay  and lamdba is the decay factor

Review a model of how word change over time to decide the best model to be used.

the chinese restaurant franchise problem explanations:

The words are the customers, the documents are the restaurant, set of document shared menu and the customers will want to sit on different tables (topic) with different set of menu (the menu here represent the table as that might be the motivation for making a customer sit). a word (customer) can either join a table (topic) that already consist other words with probabiility  (number of  words currently in the topic / position of word (customer position if it is assumed that there is a queue or stream of customer entering the restaurant)-1 +  a parameter, alpha, that regulates the way people join restaurant) or the word (customer) can chose to join a new table with the probability (alpha/position of word -1 + alpha)  (new menu from the global restaurant menu).

The chinese restuarant problem enables the topic to be unbounded, but still does not measure the evolution of topics.

A topic is considered dead when it is unused for some consecutive epoch.

The work here was built  on ahmed and Xing 2008.


The Chinese Restaurant

The first customer alwasy choose the first table

{{./pasted_image.png}}


conditional chain rule can be used to determine join probability of variables. the concept of exchangeability works across board, within and without the clusters so far the number of siting arrangement is made. for example, the probability of different permutation of clusters with 3,3,4 element will always be the same irrespective of the arrangement . therefore, this could form a distribution of the partition of n, where n is the number of customer.


when applying this concept to 

but from the intuition on CRP , it is not going to form a cluster at the end of the day, like each data point belong to one partition

**Topics over Time: A Non-Markov Continuous-Time Model of Topical Trends**
Topic models , presents a low dimension muti-facet model of document, which could occur as a co-occurendce models of words (LDA),  words and research paper citation, word sequences with markov dependencies (mixed membership model of scientific publication in this paper words in abstract and bibiligraphy are treated as different mixtures), word sequences and markov dependencies (this is basically like trying to  infused the semantic gnerating by topic model with syntactic generation by markov model),word and author and , social network and messaging cases.

This paper model time jointly with the word occurrence, meaning that part of the observed variables in document is the timestamp on the document. It does not use a state space model or markov assumption that involves some form of transition matrix. Topic are responsible for generating not just the word co-occurence but also the time attached with each topic. This models solves the problem of dynamic epoch aside from likely computation complexity that i imagine might occur. Even this model can be used to model death and birth of topics,

"The model’s generative storyline can be understood in two
different ways. We fit the model parameters according to a
generative model in which a per-document multinomial dis-
tribution over topics is sampled from a Dirichlet, then for
each word occurrence we sample a topic; next a per-topic
multinomial generates the word, and a per-topic Beta dis-
tribution generates the document’s time stamp."

Therefore, timestamp are associated with each word in a document, since the document carry the timestamp and the timestamp is observed.  alternative case is when the timestamp i s only attached to the document (the timestamp will need to be normalised  in some form, a small timestamp like let say minutes could  increase the complexity of the model ). The continous model can as well model the dynamism based on the content in different part of the document, attaching the timestamp to the word will allow for this flexibility. 

However, this paper does not capture the movement of topic overtime as this is somewhat fixed, but only captures the co-occurence of word in a topic in the given time (that is words are not getting added or droping from the topic). The point for this is not to allow some form of noise in the dynamic model. for example:

"
Imagine a subset of documents containing strong
co-occurrence patterns across time: first between birds and
aerodynamics, then aerodynamics and heat, then heat and
quantum mechanics—this could lead to a single topic that
follows this trajectory, and lead the user to inappropriately
conclude that birds and quantum mechanics are time-shifted
versions of the same topic
".

The whole concept in this paper is like indexing topic distribution by timestamp and applied beta distritution  and Gibbs sampling


**Dynamic Mixture Models for Multiple Time Series**

The generative model is similar to that of LDA except the parameter as sample from that generated from the previous time t-1 except for the first one that is generated from a dirchlet distribution. the models is applied to stream of text data.

{{./pasted_image001.png}}

other things to review :

bursty  : used to describe computer data that is sent in short, sudden periods of activity: 


**A nonparametric mixture model for topic modeling over time**
